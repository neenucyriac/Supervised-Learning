{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2edee-9aa0-40a0-ad7e-a2a686f90a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac393fe1-2981-4c8e-979b-1182977d7a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean radius                0\n",
      "mean texture               0\n",
      "mean perimeter             0\n",
      "mean area                  0\n",
      "mean smoothness            0\n",
      "mean compactness           0\n",
      "mean concavity             0\n",
      "mean concave points        0\n",
      "mean symmetry              0\n",
      "mean fractal dimension     0\n",
      "radius error               0\n",
      "texture error              0\n",
      "perimeter error            0\n",
      "area error                 0\n",
      "smoothness error           0\n",
      "compactness error          0\n",
      "concavity error            0\n",
      "concave points error       0\n",
      "symmetry error             0\n",
      "fractal dimension error    0\n",
      "worst radius               0\n",
      "worst texture              0\n",
      "worst perimeter            0\n",
      "worst area                 0\n",
      "worst smoothness           0\n",
      "worst compactness          0\n",
      "worst concavity            0\n",
      "worst concave points       0\n",
      "worst symmetry             0\n",
      "worst fractal dimension    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Loading the Dataset:\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "\n",
    "#Handling Missing Values:\n",
    "print(X.isnull().sum())\n",
    "\n",
    "\n",
    "#Feature Scaling: \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#Justification for Preprocessing:\n",
    "\n",
    "'''Scaling: This ensures that all features contribute equally to the distance metrics used by certain algorithms (like k-NN and SVM).\n",
    "Without scaling, features with larger ranges can disproportionately influence the model performance.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad8c88d-6b8d-4a5c-8424-50d839435fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#1. Logistic Regression: \n",
    "'''Logistic Regression is a linear model used for binary classification.\n",
    "It predicts the probability of a class by fitting a logistic function to the data.'''\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "log_reg_accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55da992-b7e8-4608-982a-415b67a786a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Decision Tree Classifier:\n",
    "'''Decision Trees split the data into subsets based on feature values. It’s interpretable and can model complex relationships.'''\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "y_pred_dt = dt_classifier.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b7e11f-614e-4333-9685-7df7acf42506",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Random Forest Classifier: \n",
    "'''Random Forest builds multiple decision trees and merges their results for improved accuracy and robustness against overfitting.'''\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c04f19-2275-4aa9-8221-e11e8a169e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Support Vector Machine (SVM):\n",
    "'''SVM finds the hyperplane that best separates classes in high-dimensional space. It’s effective for datasets with clear margins of separation.'''\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "y_pred_svm = svm_classifier.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b087f28-0897-41ac-8c38-e38f09c5f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. k-Nearest Neighbors (k-NN):\n",
    "'''k-NN classifies based on the majority class among its k nearest neighbors in the feature space. It’s simple and works well for small datasets.'''\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "y_pred_knn = knn_classifier.predict(X_test)\n",
    "knn_accuracy = accuracy_score(y_test, y_pred_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7969df-6a5c-4538-a54f-ac6d1e9451a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised-Learning\n",
    "\n",
    "#3. Model Comparison (2 marks)\n",
    "#1. Compare the performance of the five classification algorithms.----\n",
    "'''Generally, Random Forest tends to perform the best due to its ensemble nature.\n",
    "Logistic Regression and SVM often perform well due to their underlying assumptions.\n",
    "Decision Trees may struggle with overfitting, and k-NN might perform the worst because it can be sensitive to the local structure of the data.'''\n",
    "   \n",
    "#3. Which algorithm performed the best and which one performed the worst? ----\n",
    "'''Compared the performance, typically finding Random Forest to be the best performer and k-NN the least effective in many cases.'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
